{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAlex Concept Matching at scale using Azure's CosmosDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the datasets (academic, financial, patents) are preprocessed and concepts are tagged for each document, they can then be uploaded to CosmosDB (NoSQL database). Following are the steps required to upload the preprocessed documents to CosmosDB and query across financial, academic, and patent datasets -   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Create an Azure Account, setup CosmosDB account, and create a database for this project. Download API keys(MASTER_KEY) and copy host URI(HOST) of the project from the \"Keys\" section of Database Settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Create respective containers for each dataset using CosmosDB API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install cosmosdb python API using ```pip install azure-cosmos```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cosmos.documents as documents\n",
    "import azure.cosmos.cosmos_client as cosmos_client\n",
    "import azure.cosmos.exceptions as exceptions\n",
    "from azure.cosmos.partition_key import PartitionKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we create a container for companies (from Refinitiv financial dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cosmos_client.CosmosClient(HOST, {'masterKey': MASTER_KEY}, user_agent=\"CosmosDBPythonQuickstart\", user_agent_overwrite=True)\n",
    "try:\n",
    "    # setup database for this sample\n",
    "    try:\n",
    "        db = client.create_database(id=DATABASE_ID)\n",
    "        print('Database with id \\'{0}\\' created'.format(DATABASE_ID))\n",
    "\n",
    "    except exceptions.CosmosResourceExistsError:\n",
    "        db = client.get_database_client(DATABASE_ID)\n",
    "        print('Database with id \\'{0}\\' was found'.format(DATABASE_ID))\n",
    "\n",
    "    # setup container for this sample\n",
    "    try:\n",
    "        container = db.create_container(id=CONTAINER_ID, partition_key=PartitionKey(path='/year'))\n",
    "        print('Container with id \\'{0}\\' created'.format(CONTAINER_ID))\n",
    "\n",
    "    except exceptions.CosmosResourceExistsError:\n",
    "        container = db.get_container_client(CONTAINER_ID)\n",
    "        print('Container with id \\'{0}\\' was found'.format(CONTAINER_ID))\n",
    "        \n",
    "except exceptions.CosmosHttpResponseError as e:\n",
    "    print('\\nrun_sample has caught an error. {0}'.format(e.message))\n",
    "\n",
    "finally:\n",
    "    print(\"\\nrun_sample done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Prepare Dataset and Upload to the respective container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) To prepare the dataset for upload, it should be loaded as a Pandas dataframe or JSON object, such that it can be uploaded to container as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load dataframe using json.load() or pd.read_csv/excel()\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) - Ensure that there are no special characters or whitespaces in columns names, such as colons, hyphens etc. Replace all special characters with an Underscore or \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in list(df):\n",
    "    old_col = cols\n",
    "    cols = cols.replace(\"\\n\",\"\")\n",
    "    cols = cols.replace(\"('|')\",\"\")\n",
    "    cols = cols.replace(\",\",\"\")\n",
    "    cols = cols.replace(\".\",\"\")\n",
    "    cols = cols.replace(\"-\",\"\")\n",
    "    cols = cols.replace(\"(\",\"_\")\n",
    "    cols = cols.replace(\")\",\"\")\n",
    "    cols = \"_\".join(cols.split(\" \"))\n",
    "    cols = cols.lower()\n",
    "    df.rename(columns={old_col:cols}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) Ensure that there are no NaN values in the dataframe/JSON. Replace them with None or remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame(df).replace({np.nan:None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iv) Although CosmosDB creates an index across all fields by default, it still requires an \"id\" key during document upload (see https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/troubleshoot-bad-request). Therefore, assign a unique id for all documents if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Function to generate a unique string index\n",
    "def generate_unique_index():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "df['id'] = df.apply(lambda row: generate_unique_index(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(v) Ensure that concept IDs are stores as list of integers. (Generated using: [Concept Mapping](concept_mapping/concept_mapping_readme.md)). An example document now looks like - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"company_name\": \"Arisglobal LLC\",\n",
    "    \"company_nation\": \"United States\",\n",
    "    \"investee_company_trbc_economic_sector\": \"Technology\",\n",
    "    \"avg_equity_per_deal_in_search_range_usd_millions\": 0,\n",
    "    \"avg_equity_per_firm_in_search_range_usd_millions\": 0,\n",
    "    \"avg_equity_per_fund_in_search_range_usd_millions\": 0,\n",
    "    \"sum_of_equity_invested_in_search_range_usd_millions\": 0,\n",
    "    \"investee_primary_sic\": \"Prepackaged Software\",\n",
    "    \"investee_company_primary_ve_industry_subgroup_2\": \"Internet Software\",\n",
    "    \"investee_company_primary_veic\": \"Other Internet Systems Software\",\n",
    "    \"investee_company_trbc_industry_group\": \"Software & IT Services\",\n",
    "    \"investee_company_trbc_industry\": \"Software\",\n",
    "    \"investee_company_trbc_business_sector\": \"Software & IT Services\",\n",
    "    \"investee_company_trbc_activity\": \"Enterprise Software\",\n",
    "    \"investee_company_naics_2022\": \"Software Publishers\",\n",
    "    \"investee_sic\": \"Prepackaged Software\",\n",
    "    \"investee_company_ve_industry_subgroup_2\": \"Internet Software\",\n",
    "    \"investee_company_ve_primary_industry_subgroup_3\": \"Other Internet Systems Software\",\n",
    "    \"investee_company_ve_industry_class\": \"Information Technology\",\n",
    "    \"year\": 2023,\n",
    "    \"investment_round_permid\": 2,\n",
    "    \"firm_investor_beid\": 1,\n",
    "    \"fund_investor_beid\": 2,\n",
    "    \"deal_value_usd_millions\": 0,\n",
    "    \"deal_value_usd_millions1\": 0,\n",
    "    \"deal_rank_value_usd_millions\": 0,\n",
    "    \"investee_company_website\": \"www.arisglobal.com\",\n",
    "    \"investee_company_status\": \"Active\",\n",
    "    \"investee_company_founded_date\": \"1987-01-01 00:00:00\",\n",
    "    \"investee_company_long_business_description\": \"ArisGlobal LLC is a United States-based technology company. The Company provides cloud-based software solutions for pharmacovigilance and drug safety, clinical development, regulatory, and medical affairs. Its drug development technology platform, LifeSphere, uses its Nava cognitive computing engine to automate core functions of the drug development lifecycle. Its LifeSphere platform includes LifeSphere Clinical, LifeSphere Regulatory, LifeShpere Safety, LifeSphere Medical Affairs, LifeShpere EasyDocs and LifeSphere Cloud. The Company services include professional services, managed services and LifeSphere trainings.\",\n",
    "    \"investee_company_short_business_description\": \"Provider of cloud-based software solutions for drug safety.\",\n",
    "    \"investee_company_alias_name\": None,\n",
    "    \"openalex_concept_ids\": [\n",
    "        79974875,\n",
    "        57658597,\n",
    "        2777904410,\n",
    "        110354214,\n",
    "        144133560,\n",
    "        195094911,\n",
    "        38652104,\n",
    "        41008148\n",
    "    ],\n",
    "    \"openalex_concept_ids_with_full_chain\": [\n",
    "        79974875,\n",
    "        57658597,\n",
    "        2777904410,\n",
    "        110354214,\n",
    "        144133560,\n",
    "        195094911,\n",
    "        38652104,\n",
    "        41008148,\n",
    "        71924100,\n",
    "        2780035454,\n",
    "        127413603,\n",
    "        98274493,\n",
    "        199360897,\n",
    "        111919701\n",
    "    ],\n",
    "    \"openalex_concepts\": [\n",
    "        \"cloud computing\",\n",
    "        \"pharmacovigilance\",\n",
    "        \"software\",\n",
    "        \"engineering management\",\n",
    "        \"business\",\n",
    "        \"process management\",\n",
    "        \"computer security\",\n",
    "        \"computer science\"\n",
    "    ],\n",
    "    \"openalex_concepts_with_chains\": [\n",
    "        \"cloud computing\",\n",
    "        \"pharmacovigilance\",\n",
    "        \"software\",\n",
    "        \"engineering management\",\n",
    "        \"business\",\n",
    "        \"process management\",\n",
    "        \"computer security\",\n",
    "        \"computer science\",\n",
    "        \"medicine\",\n",
    "        \"drug\",\n",
    "        \"engineering\",\n",
    "        \"pharmacology\",\n",
    "        \"programming language\",\n",
    "        \"operating system\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(vi) Convert the preprocessed dataframe to list of dictionaries, upload each item(dictionary) using predefined create_item() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_dict = df.to_dict(orient='records')    \n",
    "\n",
    "count = 0 # To show upload status\n",
    "for item in companies_dict:\n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "    container.create_item(body=item)\n",
    "    count+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that these steps have been completed for all datasets. The following steps will assume 3 containers in the database (Companies, Research Papers, Patent Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Query the CosmosDB containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL query below returns the company of details of companies containing OpenAlex concept Ids 2777904410 and 144133560 by quering the companies container we created on CosmosDB. Using the company IDs returned in this query, aggregated investment amounts for these companies can be fetched using a groupby query on investments container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_items(container, company_name):\n",
    "    print('\\nQuerying for an  Item by Partition Key\\n')\n",
    "    # Including the partition key value of account_number in the WHERE filter results in a more efficient query\n",
    "    items = list(container.query_items(\n",
    "        query=\"SELECT * FROM companies WHERE ARRAY_CONTAINS(companies.openalex_concept_ids,[2777904410,144133560],True)\",enable_cross_partition_query=True\n",
    "    ))\n",
    "    # print('Item queried by Partition Key {0}'.format(items[0].get(\"id\")))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similar query can be performed for other containers to fetch academic and patent documents pertaining to specific concept IDs. We can then perform aggregations, joins and compare trends for research papers, patents and investment rounds belonging to the same concept IDs, shown in [Webapp Readme](web_application/web_application_readme.md).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
